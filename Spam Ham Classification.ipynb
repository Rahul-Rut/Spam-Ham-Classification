{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the libraries of pandas, numpy, re, sklearn models and count vectorizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determining the number of folds, set to 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the dataset in dataframe, and naming the columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_table('https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv', header = None)\n",
    "df.columns = ['Category','Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining a function to convert the given sentences into a sparse matrix, and return it as a dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vect(raw_x_train):\n",
    "    vectorizer = CountVectorizer( )\n",
    "    X = vectorizer.fit_transform(raw_x_train.tolist())\n",
    "    return(pd.DataFrame(X.toarray(),index = raw_x_train.index, columns = vectorizer.get_feature_names()), vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining a function to calculate the required probabilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary(x_train, y_train):\n",
    "    dict1 = (y_train.groupby(y_train).size()/y_train.shape[0]).to_dict() #Calculating probabilties of Each Test Class Occurrence\n",
    "    didict = {}   #Initializing the Dictionary\n",
    "    for i in dict1:   #For each class\n",
    "        denom = (  (x_train.loc[(y_train[y_train == i].index)].values.sum()) + (len(x_train.columns))   ) #Calculating the denominator for NB. Added the length too for Laplace Smoothing\n",
    "        didict[i] = (((x_train.loc[(y_train[y_train == i].index)]).sum()) + 1  )/denom  #Under each test class, storing the independent probability for each word or train column given the considered test class \n",
    "    return(dict1, didict)  # returning both these classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining a function to predict the values/ probabilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_val(x_test, dict1, didict):\n",
    "    predict_list1 = np.empty((0,2))   # Initializing a np array to store prediction values\n",
    "    for sentence1 in x_test:          #For every sentence(value) in the test dataset\n",
    "        list1 = []                    #Initializing a list to store values of each sentence\n",
    "        for classes in dict1:         #For every class in our dictionary\n",
    "            val = 1                   #Initializing a value with 1 so as it gets mutiplied \n",
    "            for word in re.findall('\\w+',sentence1):              #For every word in the sentence(value)\n",
    "                try:\n",
    "                    val = val * didict[classes][word.lower()]     #Lowercasing the word, then multilying the respective probability for that specific class\n",
    "                except Exception:\n",
    "                    pass                                          #Passing in case, the word is not found in our training set\n",
    "            (list1.append(val*dict1[classes]))                    #Storing both(one of each class) the probabilities in the list \n",
    "            #print(len(list1))\n",
    "        predict_list1 = np.append(predict_list1,np.array([list1]), axis = 0)   #Storing the above lists (one for each sentence) in our np array\n",
    "    return(predict_list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining a function to translate the values to class labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(predict_list1, dict1):\n",
    "    list2 = []\n",
    "    for i in range(predict_list1.shape[0]):\n",
    "        list2.append(list(dict1)[np.where(predict_list1[i]==np.max(predict_list1[i]))[0][0]]) #Returns the index of the maximum value in each row, substitutes in dictionary, and returns the name of the class\n",
    "    return(np.array(list2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A function that brings together all the functions above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp(df):\n",
    "    x = df['Text']\n",
    "    y = df['Category']\n",
    "    final_predictions = pd.Series(index = df.index)\n",
    "    for train,test in skf.split(x,y):\n",
    "        x_train = x[train]\n",
    "        y_train = y[train]\n",
    "        x_train_df, vectorizer = vect(x_train)\n",
    "        class_dict, model_dict = dictionary(x_train_df, y_train)\n",
    "        print(class_dict)\n",
    "        x_test = x[test]\n",
    "        y_test = y[test]\n",
    "        predict_probs = predict_val(x_test, class_dict , model_dict)\n",
    "        predictions = predict_class(predict_probs, class_dict)\n",
    "        final_predictions.loc[test] = predictions\n",
    "        accuracy = (predictions == y_test).sum()/len(y_test)\n",
    "    final_acc = (final_predictions == y).sum()/ len(y)\n",
    "    print(final_acc)\n",
    "    return(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ham': 0.8659127625201939, 'spam': 0.13408723747980614}\n",
      "{'ham': 0.8659488559892329, 'spam': 0.13405114401076715}\n",
      "{'ham': 0.8659488559892329, 'spam': 0.13405114401076715}\n",
      "0.9860014357501795\n"
     ]
    }
   ],
   "source": [
    "new_df = nlp(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
